{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8A_2oW1fUKPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df52ee7-ade2-4ee2-efa7-479f95110679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mounting Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate, num_epochs):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        # Initializing weights and bias\n",
        "        self.weights = np.zeros(X_train.shape[1])\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i in range(X_train.shape[0]):\n",
        "                # Calculating the prediction\n",
        "                prediction = np.dot(X_train[i], self.weights) + self.bias\n",
        "\n",
        "                # Updating the weights and bias based on the prediction error\n",
        "                if int(y_train[i]) * prediction <= 0:  # Cast y_train[i] to int\n",
        "                    self.weights += self.learning_rate * int(y_train[i]) * X_train[i]\n",
        "                    self.bias += self.learning_rate * int(y_train[i])\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Predicting class labels for test data\n",
        "        predictions = np.dot(X_test, self.weights) + self.bias\n",
        "        return np.sign(predictions)\n",
        "\n",
        "def main():\n",
        "    # Loading the dataset\n",
        "    dataset_path = \"/content/drive/MyDrive/csie.ntu.edu.tw_~cjlin_libsvmtools_datasets_binary_diabetes.csv\"\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(dataset_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            label = int(parts[0])  # Extract the class label\n",
        "            features = [float(part.split(':')[1]) for part in parts[1:]]  # Extract and convert features\n",
        "            labels.append(label)\n",
        "            data.append(features)\n",
        "\n",
        "    X = np.array(data)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # Spliting the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Defining the hyperparameters\n",
        "    learning_rate = 0.1\n",
        "    num_epochs = 100\n",
        "\n",
        "    # Initializing and training the Perceptron model\n",
        "    perceptron = Perceptron(learning_rate, num_epochs)\n",
        "    perceptron.train(X_train, y_train)\n",
        "\n",
        "    # Making predictions on the test set\n",
        "    y_pred = perceptron.predict(X_test)\n",
        "\n",
        "    # Evaluating the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MwXlfsecjeq",
        "outputId": "24d7c96d-1558-43b0-b09f-a3474e8d154b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate, num_epochs):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        # Initializing weights and bias\n",
        "        self.weights = np.zeros(X_train.shape[1])\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i in range(X_train.shape[0]):\n",
        "                # Calculating the prediction\n",
        "                prediction = np.dot(X_train[i], self.weights) + self.bias\n",
        "\n",
        "                # Updating the weights and bias based on the prediction error\n",
        "                if int(y_train[i]) * prediction <= 0:  # Cast y_train[i] to int\n",
        "                    self.weights += self.learning_rate * int(y_train[i]) * X_train[i]\n",
        "                    self.bias += self.learning_rate * int(y_train[i])\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Predicting class labels for test data\n",
        "        predictions = np.dot(X_test, self.weights) + self.bias\n",
        "        return np.sign(predictions)\n",
        "\n",
        "def main():\n",
        "    # Loading the dataset\n",
        "    dataset_path = \"/content/drive/MyDrive/csie.ntu.edu.tw_~cjlin_libsvmtools_datasets_binary_diabetes.csv\"\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(dataset_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            label = int(parts[0])  # Extract the class label\n",
        "            features = [float(part.split(':')[1]) for part in parts[1:]]  # Extract and convert features\n",
        "            labels.append(label)\n",
        "            data.append(features)\n",
        "\n",
        "    X = np.array(data)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # Spliting the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Defining hyperparameters\n",
        "    learning_rate = 0.01\n",
        "    num_epochs = 750\n",
        "\n",
        "    # Initializing and train the Perceptron model\n",
        "    perceptron = Perceptron(learning_rate, num_epochs)\n",
        "    perceptron.train(X_train, y_train)\n",
        "\n",
        "    # Making predictions on the test set\n",
        "    y_pred = perceptron.predict(X_test)\n",
        "\n",
        "    # Evaluating the model (you can use metrics like accuracy, precision, recall, etc.)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d660a71-d38d-439b-8a3d-eeee1323a9d7",
        "id": "A2e6AxOZLxnB"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7597402597402597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import Perceptron\n",
        "from itertools import product\n",
        "\n",
        "# Loading the dataset\n",
        "dataset_path = \"/content/drive/MyDrive/csie.ntu.edu.tw_~cjlin_libsvmtools_datasets_binary_diabetes.csv\"\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "with open(dataset_path, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split()\n",
        "        label = int(parts[0])  # Extract the class label\n",
        "        features = [float(part.split(':')[1]) for part in parts[1:]]  # Extract and convert features\n",
        "        labels.append(label)\n",
        "        data.append(features)\n",
        "\n",
        "X = np.array(data)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining candidate hyperparameters\n",
        "learning_rates = [0.01, 0.1, 0.5 , 0.01]\n",
        "num_epochs_values = [500, 750, 1000 ,750]\n",
        "\n",
        "# Creating a list for results\n",
        "results = []\n",
        "\n",
        "# Iterating through all combinations of hyperparameters\n",
        "for learning_rate, num_epochs in product(learning_rates, num_epochs_values):\n",
        "\n",
        "    perceptron = Perceptron(eta0=learning_rate, max_iter=num_epochs, random_state=42)\n",
        "    perceptron.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = perceptron.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results.append((learning_rate, num_epochs, accuracy))\n",
        "\n",
        "for learning_rate, num_epochs, accuracy in results:\n",
        "    print(f\"Learning Rate: {learning_rate}, Num Epochs: {num_epochs}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "a8nPKMtIPqSJ",
        "outputId": "7d392e1d-4359-49cc-8374-59583a5da594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.01, Num Epochs: 500, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.01, Num Epochs: 750, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.01, Num Epochs: 1000, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.01, Num Epochs: 750, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.1, Num Epochs: 500, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.1, Num Epochs: 750, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.1, Num Epochs: 1000, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.1, Num Epochs: 750, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.5, Num Epochs: 500, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.5, Num Epochs: 750, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.5, Num Epochs: 1000, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.5, Num Epochs: 750, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.01, Num Epochs: 500, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.01, Num Epochs: 750, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.01, Num Epochs: 1000, Accuracy: 0.6493506493506493\n",
            "Learning Rate: 0.01, Num Epochs: 750, Accuracy: 0.6493506493506493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate, num_epochs):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.weights = np.zeros(X_train.shape[1])\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i in range(X_train.shape[0]):\n",
        "                prediction = np.dot(X_train[i], self.weights) + self.bias\n",
        "\n",
        "                if int(y_train[i]) * prediction <= 0:  # Cast y_train[i] to int\n",
        "                    self.weights += self.learning_rate * int(y_train[i]) * X_train[i]\n",
        "                    self.bias += self.learning_rate * int(y_train[i])\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = np.dot(X_test, self.weights) + self.bias\n",
        "        return np.sign(predictions)\n",
        "\n",
        "def main():\n",
        "    learning_rate = 0.01\n",
        "    num_epochs = 750\n",
        "\n",
        "    dataset_path = \"/content/drive/MyDrive/csie.ntu.edu.tw_~cjlin_libsvmtools_datasets_binary_diabetes.csv\"\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(dataset_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            label = int(parts[0])\n",
        "            features = [float(part.split(':')[1]) for part in parts[1:]]  # Extract and convert features\n",
        "            labels.append(label)\n",
        "            data.append(features)\n",
        "\n",
        "    X = np.array(data)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # Spliting the data into training and testing sets\n",
        "    kf = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "    # Performing 5-fold cross-validation\n",
        "    scores = []\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        perceptron = Perceptron(learning_rate, num_epochs)\n",
        "        perceptron.train(X_train, y_train)\n",
        "\n",
        "        y_pred = perceptron.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        scores.append(accuracy)\n",
        "\n",
        "        print(\"Fold {} accuracy: {}\".format(kf.get_n_splits(), accuracy))\n",
        "\n",
        "    mean_accuracy = np.mean(scores)\n",
        "    print(\"Mean accuracy:\", mean_accuracy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2MvomvKQQpF",
        "outputId": "ddc83736-08ab-441b-9763-c3261575aabd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 accuracy: 0.5454545454545454\n",
            "Fold 5 accuracy: 0.5324675324675324\n",
            "Fold 5 accuracy: 0.7012987012987013\n",
            "Fold 5 accuracy: 0.6928104575163399\n",
            "Fold 5 accuracy: 0.5359477124183006\n",
            "Mean accuracy: 0.601595789831084\n"
          ]
        }
      ]
    }
  ]
}